{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c9c185",
   "metadata": {},
   "source": [
    "# LSTM Code - Multiple encoding methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16c76f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b> We will implement LSTM and in next python file we will implement LSTM with \n",
    "\n",
    "<b> 1. GLVOE \n",
    "    \n",
    "<b> 2. CBOW \n",
    "    \n",
    "<b> 3. Skipgram </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297d86e",
   "metadata": {},
   "source": [
    "The data set was taken from UCI Machine Learning Repo\n",
    "It is a public set of comments collected for spam research. It has five datasets composed by 1,956 real messages extracted from five videos that were among the 10 most viewed on the collection period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66c06b",
   "metadata": {},
   "source": [
    "Alberto, T. & Lochter, J. (2015). YouTube Spam Collection [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C58885."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c31bd",
   "metadata": {},
   "source": [
    "## 1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e9467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sys, re, bs4, nltk, sklearn, matplotlib\n",
    "from bs4 import BeautifulSoup\n",
    "import datasets, transformers \n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,classification_report\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c0627",
   "metadata": {},
   "source": [
    "## 2. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0a0ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\ashutosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\ashutosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ucimlrepo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\ashutosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ucimlrepo) (2024.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\ashutosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashutosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ashutosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ashutosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashutosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Ashutosh\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d895c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 380, 'name': 'YouTube Spam Collection', 'repository_url': 'https://archive.ics.uci.edu/dataset/380/youtube+spam+collection', 'data_url': 'https://archive.ics.uci.edu/static/public/380/data.csv', 'abstract': 'It is a public set of comments collected for spam research. It has five datasets composed by 1,956 real messages extracted from five videos that were among the 10 most viewed on the collection period.', 'area': 'Computer Science', 'tasks': ['Classification'], 'characteristics': ['Text'], 'num_instances': 1956, 'num_features': 3, 'feature_types': [], 'demographics': [], 'target_col': ['CLASS'], 'index_col': ['VIDEO', 'COMMENT_ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2015, 'last_updated': 'Wed Apr 03 2024', 'dataset_doi': '10.24432/C58885', 'creators': ['T.C. Alberto', 'J.V. Lochter'], 'intro_paper': None, 'additional_info': {'summary': 'The table below lists the datasets, the YouTube video ID, the amount of samples in each class and the total number of samples per dataset.\\r\\n\\r\\nDataset --- YouTube ID -- # Spam - # Ham - Total\\r\\nPsy ------- 9bZkp7q19f0 --- 175 --- 175 --- 350\\r\\nKatyPerry - CevxZvSJLk8 --- 175 --- 175 --- 350\\r\\nLMFAO ----- KQ6zr6kCPj8 --- 236 --- 202 --- 438\\r\\nEminem ---- uelHwf8o7_U --- 245 --- 203 --- 448\\r\\nShakira --- pRpeEdMmmQ0 --- 174 --- 196 --- 370\\r\\n\\r\\nNote: the chronological order of the comments were kept.\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The collection is composed by one CSV file per dataset, where each line has the following attributes:\\r\\n\\r\\nCOMMENT_ID,AUTHOR,DATE,CONTENT,TAG\\r\\n\\r\\nWe offer one example bellow:\\r\\n\\r\\nz12oglnpoq3gjh4om04cfdlbgp2uepyytpw0k,Francisco Nora,2013-11-28T19:52:35,please like :D https://premium.easypromosapp.com/voteme/19924/616375350,1\\r\\n', 'citation': None}}\n",
      "         name     role         type demographic description units  \\\n",
      "0       VIDEO       ID  Categorical        None        None  None   \n",
      "1  COMMENT_ID       ID  Categorical        None        None  None   \n",
      "2      AUTHOR  Feature  Categorical        None        None  None   \n",
      "3        DATE  Feature  Categorical        None        None  None   \n",
      "4     CONTENT  Feature  Categorical        None        None  None   \n",
      "5       CLASS   Target       Binary        None        None  None   \n",
      "\n",
      "  missing_values  \n",
      "0             no  \n",
      "1             no  \n",
      "2             no  \n",
      "3             no  \n",
      "4             no  \n",
      "5             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "youtube_spam_collection = fetch_ucirepo(id=380) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = youtube_spam_collection.data.features \n",
    "y = youtube_spam_collection.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(youtube_spam_collection.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(youtube_spam_collection.variables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb808ebb",
   "metadata": {},
   "source": [
    "## 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a124f9",
   "metadata": {},
   "source": [
    "We will prepeocess the data, tokenize, clean it (stemming, lemmization), encode it (bag of words, tf-idf), then use multiple methodes to classify it and compare resutls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4e1b7",
   "metadata": {},
   "source": [
    "    ```Input data\n",
    "    pre-processing\n",
    "        A --> Toeknization\n",
    "        B --> Stemming/Lemmization\n",
    "        C --> Encoding (Bag of Words, TF-IDF)\n",
    "    Create Embeddings\n",
    "        D --> GLOVE\n",
    "        E --> Word2Vec- CBOW\n",
    "        F --> Word2Vec - Skipgram\n",
    "    DL Model\n",
    "        G. --> Define model\n",
    "        H. --> Train model\n",
    "        I. --> Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba87e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huh, anyway check out this you[tube] channel: kobyoshi02\n",
      "Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!\n",
      "just for test I have to say murdev.com\n",
      "me shaking my sexy ass on my channel enjoy ^_^ ﻿\n",
      "watch?v=vtaRGgvGtWQ   Check this out .﻿\n"
     ]
    }
   ],
   "source": [
    "for i in X['CONTENT'][0:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b10efa0",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9516592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashutosh\\AppData\\Local\\Temp\\ipykernel_12948\\787672344.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  X_all = [BeautifulSoup(i, \"html.parser\").get_text() for i in X_all]\n"
     ]
    }
   ],
   "source": [
    "# We want to remove URL as comment as it will confuse the model but we will keep a flag that a url watch present here, \n",
    "#as it might be a strong indicator of a spam comment\n",
    "\n",
    "X_all = [re.sub(r'http\\S+|www\\S+|https\\S+', 'url', i, flags=re.MULTILINE) for i in X['CONTENT']]\n",
    "\n",
    "# Similarly for watch, we will remove complext suffix but keep the word watch as an input to out model\n",
    "X_all = [re.sub(r'watch\\?v=\\S+', 'watch', i, flags=re.MULTILINE) for i in X_all]\n",
    "\n",
    "# Remove any html tags by mistake\n",
    "X_all = [BeautifulSoup(i, \"html.parser\").get_text() for i in X_all]\n",
    "\n",
    "# for any emoji or emoticon replace it with the word \"emoji\" as it can be a useful feature\n",
    "X_all = [re.sub(\"[:;][)|(DP]\",\"emoji\",i) for i in X_all]\n",
    "\n",
    "# any number does not look like year should be replaced with an identifier number\n",
    "\n",
    "def not_number(string):\n",
    "    string = re.sub(\",\",\"\",string)\n",
    "    try:\n",
    "        float(string)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True\n",
    "    \n",
    "def check_range(string_no):\n",
    "    string_no = re.sub(\",\",\"\",string_no)\n",
    "    if(float(string_no) <= 2100.0 and float(string_no) >= 1800.0):\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def remove_suprious_num(snt):\n",
    "    token_words = word_tokenize(snt)\n",
    "    changed_words = [i if not_number(i) else \"year\" if  check_range(i) else \"large number\" for i in token_words]\n",
    "    return(\" \".join(changed_words))\n",
    "\n",
    "X_all = [remove_suprious_num(i) for i in X_all]\n",
    "\n",
    "# convert all to lower case\n",
    "X_all = [i.casefold() for i in X_all]\n",
    "\n",
    "# romve extra white space \n",
    "X_all = [re.sub(r'[^\\w\\s]', '', i) for i in X_all]\n",
    "\n",
    "X_all = [re.sub(r'\\s+', ' ', i).strip() for i in X_all]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f5559",
   "metadata": {},
   "source": [
    "## 4. Split into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7111e77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "1        1005\n",
       "0         951\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us check the distirbution of labels, if it is highly imbalanced\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491828d",
   "metadata": {},
   "source": [
    "The labels seem to be very well balanced, hence we need not worry about sampling techniques (oversampling/undersampling/smote,etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b8d1d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use y to stratify so the distribution of labels is simialr in training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f277e8",
   "metadata": {},
   "source": [
    "Let us check the distribution in training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a929767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS\n",
      "1        804\n",
      "0        760\n",
      "Name: count, dtype: int64\n",
      "CLASS\n",
      "1        201\n",
      "0        191\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e816a8f",
   "metadata": {},
   "source": [
    "## 5. Pre-processing and tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f0ad2",
   "metadata": {},
   "source": [
    "### 5a. Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96309c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(stopwords)\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87714fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sent):\n",
    "    token_words = word_tokenize(sent)\n",
    "    clean_token_words = [i for i in token_words if i not in stop_words]\n",
    "    return(\" \".join(clean_token_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e719ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video large number views large number million people earth',\n",
       " 'head like large number years ago time flies',\n",
       " 'large number billions year',\n",
       " 'wtf subscribe channel thanx emoji',\n",
       " 'omg',\n",
       " 'please become first subscriber thank',\n",
       " 'everyone come check new gta large number gameplay right watch',\n",
       " 'remember back popular everyone school shuffling crazy',\n",
       " 'url please halp project',\n",
       " 'haha funny see salt westerners top views youtube goes video dont even understand keep salt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [remove_stop_words(i) for i in X_train]\n",
    "X_test = [remove_stop_words(i) for i in X_test]\n",
    "X_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e988ab",
   "metadata": {},
   "source": [
    "### 5b. Normalization - Lemmization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5e6492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video large number view large number million people earth',\n",
       " 'head like large number year ago time fly',\n",
       " 'large number billion year',\n",
       " 'wtf subscribe channel thanx emoji',\n",
       " 'omg',\n",
       " 'please become first subscriber thank',\n",
       " 'everyone come check new gta large number gameplay right watch',\n",
       " 'remember back popular everyone school shuffling crazy',\n",
       " 'url please halp project',\n",
       " 'haha funny see salt westerner top view youtube go video dont even understand keep salt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_data(sent,lemmitizer):\n",
    "    token_words = word_tokenize(sent)\n",
    "    clean_tokens = [lemmitizer.lemmatize(i) for i in token_words]\n",
    "    return(\" \".join(clean_tokens)) \n",
    "\n",
    "lemmitizer = WordNetLemmatizer()\n",
    "X_train = [normalize_data(i,lemmitizer) for i in X_train]\n",
    "X_test = [normalize_data(i,lemmitizer) for i in X_test]\n",
    "X_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31db3f9",
   "metadata": {},
   "source": [
    "# Embedding - Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d586e",
   "metadata": {},
   "source": [
    "<b> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.We will use a pre-trained golve model on wikipedia data. We will load it and use it to create embeddings for our corpus.\\\n",
    "\\\n",
    "To download the pre-trained model and use golve refer to - https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64c176ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_dict):\n",
    "    model = {}\n",
    "    with open(glove_dict, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word = lemmitizer.lemmatize(word)\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            model[word] = vector\n",
    "    return model\n",
    "\n",
    "glove_model = load_glove_model('./glove.6B.50d.txt')  # Example for 50-dimensional vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fdd117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embedding(sent):\n",
    "    emb_vec = []\n",
    "    tokens = word_tokenize(sent)\n",
    "    for token in tokens:\n",
    "        temp_vec = glove_model.get(token, np.zeros(50))\n",
    "        emb_vec.append(temp_vec)\n",
    "    return(emb_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e9511ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_embed = [get_glove_embedding(i) for i in X_train]\n",
    "X_test_glove_embed = [get_glove_embedding(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16adfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_vectors(vectors_list, max_len_sentence, embedding_size):\n",
    "    padded_vectors = []\n",
    "    for vecs in vectors_list:\n",
    "        if len(vecs) < max_len_sentence:\n",
    "            pad_length = max_len_sentence - len(vecs)\n",
    "            vecs.extend([np.zeros(embedding_size)] * pad_length)\n",
    "        else:\n",
    "            vecs = vecs[:max_len_sentence]\n",
    "        padded_vectors.append(vecs)\n",
    "    return np.array(padded_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a421acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1564, 25, 50)\n"
     ]
    }
   ],
   "source": [
    "padded_embeddings_X_train_glove = pad_sentence_vectors(X_train_glove_embed,25,50)\n",
    "padded_embeddings_X_test_glove = pad_sentence_vectors(X_test_glove_embed,25,50)\n",
    "print(padded_embeddings_X_train_glove.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362140c",
   "metadata": {},
   "source": [
    "## Create training tensor dataset  for GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "579d0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1564, 25, 50])\n",
      "torch.Size([1564, 50, 25])\n",
      "torch.Size([392, 25, 50])\n",
      "torch.Size([392, 50, 25])\n"
     ]
    }
   ],
   "source": [
    "X_tensor_train = torch.tensor(padded_embeddings_X_train_glove, dtype=torch.float32)    \n",
    "print(X_tensor_train.shape)\n",
    "\n",
    "#The expected sequence of nn.Conv1d is (batch_size, channels, sequence_length)\n",
    "# hence we change the shape (1564, 15, 50) to (1564, 50, 15)\n",
    "X_tensor_train = X_tensor_train.permute(0, 2, 1)\n",
    "print(X_tensor_train.shape)\n",
    "\n",
    "y_tensor_train = torch.tensor(np.array(y_train), dtype=torch.long)\n",
    "\n",
    "dataset_train = TensorDataset(X_tensor_train, y_tensor_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=10, shuffle=False,drop_last=False)\n",
    "\n",
    "\n",
    "X_tensor_test = torch.tensor(padded_embeddings_X_test_glove, dtype=torch.float32)    \n",
    "print(X_tensor_test.shape)\n",
    "\n",
    "#The expected sequence of nn.Conv1d is (batch_size, channels, sequence_length)\n",
    "# hence we change the shape (1564, 15, 50) to (1564, 50, 15)\n",
    "X_tensor_test = X_tensor_test.permute(0, 2, 1)\n",
    "print(X_tensor_test.shape)\n",
    "\n",
    "y_tensor_test = torch.tensor(np.array(y_test), dtype=torch.long)\n",
    "\n",
    "dataset_test = TensorDataset(X_tensor_test, y_tensor_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9c380",
   "metadata": {},
   "source": [
    "# Embedding - CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbec71e",
   "metadata": {},
   "source": [
    "CBOW or Continous Bag of Words is word embedding technique. It creates embedding by trying to predict the word by using surrounding words. Surrounding is defined by context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4b762cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'video large number view large number million people earth'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14271f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us tokenize words\n",
    "train_corpus = [word_tokenize(i) for i in X_train]\n",
    "test_corpus = [word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040b746",
   "metadata": {},
   "source": [
    "First we need to create a dictionary with frequency. We will use it to filter for words with atleast >= 5 frequency and for rare word we change in UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "403b3c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = {}\n",
    "for i in train_corpus:\n",
    "    for j in i:\n",
    "        if j not in all_words:\n",
    "            all_words[j] = 1\n",
    "        else:\n",
    "            all_words[j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5823023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dict \n",
    "filtered_all_words = {i:j for i,j in all_words.items() if j >= 3 }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e59f51",
   "metadata": {},
   "source": [
    "<b> Let us create 2 indexes, word to index and index to words </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1cf1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {\n",
    "    '<PAD>': 0,\n",
    "    '<UNK>': 1}\n",
    "for i in filtered_all_words.keys():\n",
    "    if (i not in word_to_idx):\n",
    "        word_to_idx[i] = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f8e9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "inx_to_word = {}\n",
    "for i,j in word_to_idx.items():\n",
    "    inx_to_word[j] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a82cd6",
   "metadata": {},
   "source": [
    "<b> Let us pad both train and test corpus to similar lengths </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05efd6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.237e+03, 1.630e+02, 6.700e+01, 4.600e+01, 3.000e+01, 1.700e+01,\n",
       "        1.000e+00, 1.000e+00, 1.000e+00, 1.000e+00]),\n",
       " array([  0. ,  10.7,  21.4,  32.1,  42.8,  53.5,  64.2,  74.9,  85.6,\n",
       "         96.3, 107. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_of_words = []\n",
    "for i in train_corpus:\n",
    "    #print(i)\n",
    "    #print(len(i))\n",
    "    len_of_words.append(len(i))\n",
    "plt.hist(len_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b840fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_padding (wrd_crps, pad_length):\n",
    "    padded_corpus = []\n",
    "    for i in wrd_crps:\n",
    "        if len(i) >= pad_length:\n",
    "            sent = i[0:pad_length]\n",
    "        else:\n",
    "            sent = i + (pad_length-len(i)) * ['<PAD>']\n",
    "        padded_corpus.append(sent)\n",
    "    return(padded_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd89b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_corpus = do_padding(train_corpus,12)\n",
    "padded_test_corpus = do_padding(test_corpus,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff549b1c",
   "metadata": {},
   "source": [
    "<b> Change the list of words corpus to list of indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "376fb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_index(wrd_crps,word_dict):\n",
    "    ret_list = []\n",
    "    for i in wrd_crps:\n",
    "        sent = [word_dict[k] if k in word_dict else 1 for k in i]\n",
    "        ret_list.append(sent)\n",
    "    return(ret_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69edb0",
   "metadata": {},
   "source": [
    "## Let us create a dataset of contex and target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a360ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "target = []\n",
    "for i in  train_corpus:   \n",
    "    sent = []\n",
    "    for j,k in enumerate(i):       \n",
    "        if j > 2 and j < len(i) - 2:\n",
    "            context = ([i[j-2],i[j-1],i[j+1],i[j+2]])\n",
    "            target.append(i[j])\n",
    "            corpus.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76e44c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7841\n",
      "7841\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74cd812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdb993d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_target = []\n",
    "for i in target:\n",
    "    if(i in word_to_idx):\n",
    "        enc = word_to_idx[i]\n",
    "    else:\n",
    "        enc = 1\n",
    "    encoded_target.append(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fc46b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_context = []\n",
    "for i in corpus:\n",
    "    encoded_neigh = []\n",
    "    for j in i:\n",
    "        if(j in word_to_idx):\n",
    "            enc = word_to_idx[j]\n",
    "        else:\n",
    "            enc = 1\n",
    "        encoded_neigh.append(enc)\n",
    "    encoded_context.append(encoded_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81783cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7841\n",
      "7841\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_context))\n",
    "print(len(encoded_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d8a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f532d153",
   "metadata": {},
   "source": [
    "## Let us create data for CBOW encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af8d9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_train_cbow = torch.tensor(encoded_context, dtype=torch.long) \n",
    "y_tensor_train_cbow = torch.tensor(encoded_target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27bb51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_cbow = TensorDataset(X_tensor_train_cbow, y_tensor_train_cbow)\n",
    "dataloader_train_cbow = DataLoader(dataset_train_cbow, batch_size=10, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "842b1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0505cee",
   "metadata": {},
   "source": [
    "# CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32c5eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, padding_idx):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_idxs):\n",
    "        #print(\"input to embed\")\n",
    "        #print(context_idxs)\n",
    "        embeds = self.embeddings(context_idxs)\n",
    "        mask = (context_idxs != self.embeddings.padding_idx).unsqueeze(-1).float()\n",
    "        masked_embeds = embeds * mask \n",
    "        lengths = mask.sum(dim=1).clamp(min=1)\n",
    "        avg_embed = masked_embeds.sum(dim=1) / lengths \n",
    "        #print(\"output of embed\")\n",
    "        #print(embeds)\n",
    "        #print(\"shape of embed output\")\n",
    "        #print(embeds.shape)\n",
    "        #avg_embed = embeds.mean(dim=1) \n",
    "        #print(\"after mean\")\n",
    "        #print(avg_embed.shape)\n",
    "        #print(\"input to linear layer\")\n",
    "        #print(avg_embed)\n",
    "        out = self.linear(avg_embed) \n",
    "        #print(\"output of linear layer\")\n",
    "        #print(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7982ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbw = CBOW(vocab_size, embedding_dim = 50, padding_idx=0)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cbw.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "978a479b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 3661.4337\n",
      "Epoch 2/20, Loss: 2509.5882\n",
      "Epoch 3/20, Loss: 1984.5742\n",
      "Epoch 4/20, Loss: 1611.0222\n",
      "Epoch 5/20, Loss: 1347.1973\n",
      "Epoch 6/20, Loss: 1169.6983\n",
      "Epoch 7/20, Loss: 1026.4545\n",
      "Epoch 8/20, Loss: 930.8497\n",
      "Epoch 9/20, Loss: 853.7886\n",
      "Epoch 10/20, Loss: 805.2388\n",
      "Epoch 11/20, Loss: 768.7215\n",
      "Epoch 12/20, Loss: 727.9563\n",
      "Epoch 13/20, Loss: 699.3661\n",
      "Epoch 14/20, Loss: 688.0523\n",
      "Epoch 15/20, Loss: 663.6880\n",
      "Epoch 16/20, Loss: 636.5288\n",
      "Epoch 17/20, Loss: 617.4927\n",
      "Epoch 18/20, Loss: 609.0713\n",
      "Epoch 19/20, Loss: 599.1746\n",
      "Epoch 20/20, Loss: 584.3016\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for context_batch, target_batch in dataloader_train_cbow:\n",
    "        \n",
    "        outputs = model_cbw(context_batch) \n",
    "        loss = loss_fn(outputs, target_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "626c1dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of my CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d575f2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8417\n"
     ]
    }
   ],
   "source": [
    "model_cbw.eval()  \n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.inference_mode():  \n",
    "    for context_batch, target_batch in dataloader_train_cbow:\n",
    "        outputs = model_cbw(context_batch)     \n",
    "        predicted = torch.argmax(outputs, dim=1)  \n",
    "        correct += (predicted == target_batch).sum().item()\n",
    "        total += target_batch.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db49b931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_cbw.embeddings(torch.tensor([4])).squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c335b993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_cbw.embeddings(torch.tensor([0])).squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec3bb4",
   "metadata": {},
   "source": [
    "## Now let us create a corpus of embedded vectors for training and testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a165a",
   "metadata": {},
   "source": [
    "Created index values corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cc2db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_corpus_encoded = []\n",
    "for i in padded_train_corpus:\n",
    "    sent = [word_to_idx[j] if j in word_to_idx else 1 for j in i]\n",
    "    padded_train_corpus_encoded.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b9e8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test_corpus_encoded = []\n",
    "for i in padded_test_corpus:\n",
    "    sent = [word_to_idx[j] if j in word_to_idx else 1 for j in i]\n",
    "    padded_test_corpus_encoded.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad91edd",
   "metadata": {},
   "source": [
    "Now change all indexes to embedding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b79343f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1306, -0.3280, -0.0750, -0.1300,  0.0642, -0.1433, -0.4171, -0.1534,\n",
       "          0.1246, -0.0748,  0.0847, -0.2814,  0.0397, -0.0774,  0.4458,  0.0449,\n",
       "         -0.0818, -0.0231, -0.5214, -0.1480, -0.0991, -0.2671,  0.0806, -0.3679,\n",
       "         -0.0903, -0.1182, -0.1862, -0.2450, -0.2729,  0.3462, -0.0665, -0.1306,\n",
       "         -0.0733,  0.2761, -0.3023, -0.1378,  0.0954, -0.2080, -0.4112, -0.2492,\n",
       "         -0.0520, -0.0233,  0.2505,  0.0764,  0.1327,  0.4381, -0.3206,  0.2613,\n",
       "         -0.0140, -0.4254]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbw.embeddings(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fc1b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train_vector = []\n",
    "for i in padded_train_corpus_encoded:\n",
    "    sent = []\n",
    "    for j in i:\n",
    "        emb_word = (model_cbw.embeddings(torch.tensor([j])).squeeze(0).tolist())\n",
    "        sent.append(emb_word)\n",
    "    embed_train_vector.append(sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ca29799",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_test_vector = []\n",
    "for i in padded_test_corpus_encoded:\n",
    "    sent = []\n",
    "    for j in i:\n",
    "        emb_word = (model_cbw.embeddings(torch.tensor([j])).squeeze(0).tolist())\n",
    "        sent.append(emb_word)\n",
    "    embed_test_vector.append(sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c6466fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550cafea",
   "metadata": {},
   "source": [
    "# Creating Training dataset for CBOW Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee73bfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1564, 12, 50])\n",
      "torch.Size([1564, 50, 12])\n",
      "torch.Size([392, 12, 50])\n",
      "torch.Size([392, 50, 12])\n"
     ]
    }
   ],
   "source": [
    "X_tensor_train_cbow = torch.tensor(embed_train_vector, dtype=torch.float32)    \n",
    "print(X_tensor_train_cbow.shape)\n",
    "\n",
    "#The expected sequence of nn.Conv1d is (batch_size, channels, sequence_length)\n",
    "# hence we change the shape (1564, 15, 50) to (1564, 50, 15)\n",
    "X_tensor_train_cbow = X_tensor_train_cbow.permute(0, 2, 1)\n",
    "print(X_tensor_train_cbow.shape)\n",
    "\n",
    "y_tensor_train = torch.tensor(np.array(y_train), dtype=torch.long)\n",
    "\n",
    "dataset_train_cbow_final = TensorDataset(X_tensor_train_cbow, y_tensor_train)\n",
    "dataloader_train_cbow_final = DataLoader(dataset_train_cbow_final, batch_size=10, shuffle=False,drop_last=False)\n",
    "\n",
    "\n",
    "X_tensor_test_cbow = torch.tensor(embed_test_vector, dtype=torch.float32)    \n",
    "print(X_tensor_test_cbow.shape)\n",
    "\n",
    "#The expected sequence of nn.Conv1d is (batch_size, channels, sequence_length)\n",
    "# hence we change the shape (1564, 15, 50) to (1564, 50, 15)\n",
    "X_tensor_test_cbow = X_tensor_test_cbow.permute(0, 2, 1)\n",
    "print(X_tensor_test_cbow.shape)\n",
    "\n",
    "y_tensor_test = torch.tensor(np.array(y_test), dtype=torch.long)\n",
    "\n",
    "dataset_test_cbow_final = TensorDataset(X_tensor_test_cbow, y_tensor_test)\n",
    "dataloader_test_cbow_final = DataLoader(dataset_test_cbow_final, batch_size=10, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792d947",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c0dc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_spam_classify(nn.Module):\n",
    "    def __init__(self, input_dim = 50, embed_dim = 50, hidden_dim = 100, output_dim = 2, n_layers=2, dropout=0.2):\n",
    "        super(LSTM_spam_classify, self).__init__()\n",
    "        #self.embed =  nn.Embedding(num_embeddings = vocab_size, embedding_dim = embed_dim)\n",
    "        # lstm layer expects (batch_size, seq_len, input_size)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.embed(x)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        last_hidden_state = h_n[-1]  \n",
    "        x = self.dropout(last_hidden_state)\n",
    "        out = self.fc(x) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba2132",
   "metadata": {},
   "source": [
    "# LSTM Model with GLOVE Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52258c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e22fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6895752158134606\n",
      "0.6458763265685671\n",
      "0.5441114033103749\n",
      "0.5080407996466205\n",
      "0.48336169835488507\n",
      "0.4675503780317914\n",
      "0.45420198142528534\n",
      "0.44938402153124474\n",
      "0.44397543095479347\n",
      "0.437213965756878\n",
      "0.4336284573195846\n",
      "0.4294320481123438\n",
      "0.4188842463550294\n",
      "0.4163253396559673\n",
      "0.41063647964008293\n",
      "0.4020929489355938\n",
      "0.406751430575635\n",
      "0.397431654843745\n",
      "0.3929959912398818\n",
      "0.38933882435226136\n",
      "0.3890811477781861\n",
      "0.38657740674390917\n",
      "0.38389354101886414\n",
      "0.38169049822790607\n",
      "0.38520637963702725\n",
      "0.37795452752215847\n",
      "0.37998580496022655\n",
      "0.37408385562241836\n",
      "0.3725186747255599\n",
      "0.37014767819434213\n",
      "0.3600863599971791\n",
      "0.3586684953017979\n",
      "0.3522737789187272\n",
      "0.3556991436156877\n",
      "0.3487977829708415\n",
      "0.3476884334472714\n",
      "0.33748576132829783\n",
      "0.3394838646528827\n",
      "0.3379926317294312\n",
      "0.3309104584845578\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_spam_classify()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_losses = []\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader_train:\n",
    "        #inputs = inputs.float()\n",
    "        #labels = labels.float()\n",
    "        #inputs = inputs.permute(0,2,1)\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(outputs, labels.squeeze(1))\n",
    "        #print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #get_norm()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(dataloader_train)\n",
    "    print(avg_train_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    #print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191967d0",
   "metadata": {},
   "source": [
    "# Let us swap the prediction method into the code that doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42770ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on training data\n",
    "\n",
    "model.eval() \n",
    "\n",
    "train_preds = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for inputs, labels in dataloader_train:  \n",
    "        outputs_train = model(inputs) \n",
    "        preds = torch.argmax(outputs_train, dim=1) \n",
    "        train_preds.extend(preds.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d2560e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for inputs, labels in dataloader_test:  \n",
    "        outputs_test = model(inputs) \n",
    "        preds = torch.argmax(outputs_test, dim=1) \n",
    "        test_preds.extend(preds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07758b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02d72740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy is 0.846\n",
      "test accuracy is 0.788\n",
      "training precision is 0.835\n",
      "test precision is 0.766\n",
      "training recall is 0.873\n",
      "test recall is 0.846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84       760\n",
      "           1       0.83      0.87      0.85       804\n",
      "\n",
      "    accuracy                           0.85      1564\n",
      "   macro avg       0.85      0.85      0.85      1564\n",
      "weighted avg       0.85      0.85      0.85      1564\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77       191\n",
      "           1       0.77      0.85      0.80       201\n",
      "\n",
      "    accuracy                           0.79       392\n",
      "   macro avg       0.79      0.79      0.79       392\n",
      "weighted avg       0.79      0.79      0.79       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "cnn_train_accuracy = accuracy_score(y_train, train_preds)\n",
    "# Testing accuracy\n",
    "cnn_test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "cnn_train_precision = precision_score(y_train, train_preds)\n",
    "cnn_test_precision = precision_score(y_test, test_preds)\n",
    "\n",
    "cnn_train_recall = recall_score(y_train, train_preds)\n",
    "cnn_test_recall = recall_score(y_test, test_preds)\n",
    "\n",
    "print(\"training accuracy is {0}\".format(round(cnn_train_accuracy,3)))\n",
    "print(\"test accuracy is {0}\".format(round(cnn_test_accuracy,3)))\n",
    "\n",
    "print(\"training precision is {0}\".format(round(cnn_train_precision,3)))\n",
    "print(\"test precision is {0}\".format(round(cnn_test_precision,3)))\n",
    "\n",
    "print(\"training recall is {0}\".format(round(cnn_train_recall,3)))\n",
    "print(\"test recall is {0}\".format(round(cnn_test_recall,3)))\n",
    "\n",
    "print(classification_report(y_train, train_preds))\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b97e5",
   "metadata": {},
   "source": [
    "# LSTM Model with CBOW Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06d999a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6840422738129925\n",
      "0.6042166564874588\n",
      "0.5055857901550402\n",
      "0.45802175979705373\n",
      "0.41882342954350127\n",
      "0.3664397383286695\n",
      "0.3281823939815828\n",
      "0.31787481220664493\n",
      "0.2732369003781847\n",
      "0.26099183754461586\n",
      "0.2222289748394945\n",
      "0.2484635910980261\n",
      "0.2076655717145438\n",
      "0.19477549467567995\n",
      "0.18484180598596858\n",
      "0.17500262515868542\n",
      "0.14796504734831442\n",
      "0.1161871700900589\n",
      "0.10174518486389973\n",
      "0.1296663056164802\n",
      "0.11311000940585687\n",
      "0.07092812583058077\n",
      "0.08792220305348895\n",
      "0.0670159558619515\n",
      "0.05837612884611508\n",
      "0.07682991373333427\n",
      "0.06277988837797574\n",
      "0.04749971480260063\n",
      "0.03454258891395708\n",
      "0.024392171333775952\n",
      "0.06128988093766977\n",
      "0.05329338686995725\n",
      "0.0624088977927351\n",
      "0.06751248120236845\n",
      "0.03049023125311775\n",
      "0.02830711540756421\n",
      "0.022240571595332066\n",
      "0.02060391557243301\n",
      "0.017780498667999398\n",
      "0.022607800845122303\n"
     ]
    }
   ],
   "source": [
    "model2 = LSTM_spam_classify()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "train_losses = []\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader_train_cbow_final:\n",
    "        #inputs = inputs.float()\n",
    "        #labels = labels.float()\n",
    "        #inputs = inputs.permute(0,2,1)\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        outputs = model2(inputs)\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(outputs, labels.squeeze(1))\n",
    "        #print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #get_norm()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(dataloader_train)\n",
    "    print(avg_train_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    #print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a22fcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on training data\n",
    "\n",
    "model2.eval() \n",
    "\n",
    "train_preds = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for inputs, labels in dataloader_train_cbow_final:  \n",
    "        outputs_train = model2(inputs) \n",
    "        preds = torch.argmax(outputs_train, dim=1) \n",
    "        train_preds.extend(preds.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e48aa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.eval() \n",
    "\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for inputs, labels in dataloader_test_cbow_final:  \n",
    "        outputs_test = model2(inputs) \n",
    "        preds = torch.argmax(outputs_test, dim=1) \n",
    "        test_preds.extend(preds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a8c7db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy is 0.846\n",
      "test accuracy is 0.758\n",
      "training precision is 0.835\n",
      "test precision is 0.743\n",
      "training recall is 0.873\n",
      "test recall is 0.806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84       760\n",
      "           1       0.83      0.87      0.85       804\n",
      "\n",
      "    accuracy                           0.85      1564\n",
      "   macro avg       0.85      0.85      0.85      1564\n",
      "weighted avg       0.85      0.85      0.85      1564\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74       191\n",
      "           1       0.74      0.81      0.77       201\n",
      "\n",
      "    accuracy                           0.76       392\n",
      "   macro avg       0.76      0.76      0.76       392\n",
      "weighted avg       0.76      0.76      0.76       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "cnn_train_accuracy = accuracy_score(y_train, train_preds)\n",
    "# Testing accuracy\n",
    "cnn_test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "cnn_train_precision = precision_score(y_train, train_preds)\n",
    "cnn_test_precision = precision_score(y_test, test_preds)\n",
    "\n",
    "cnn_train_recall = recall_score(y_train, train_preds)\n",
    "cnn_test_recall = recall_score(y_test, test_preds)\n",
    "\n",
    "print(\"training accuracy is {0}\".format(round(cnn_train_accuracy,3)))\n",
    "print(\"test accuracy is {0}\".format(round(cnn_test_accuracy,3)))\n",
    "\n",
    "print(\"training precision is {0}\".format(round(cnn_train_precision,3)))\n",
    "print(\"test precision is {0}\".format(round(cnn_test_precision,3)))\n",
    "\n",
    "print(\"training recall is {0}\".format(round(cnn_train_recall,3)))\n",
    "print(\"test recall is {0}\".format(round(cnn_test_recall,3)))\n",
    "\n",
    "print(classification_report(y_train, train_preds))\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3e4f3",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac0d57",
   "metadata": {},
   "source": [
    "We can observe overfitting to be an issue with CBOW as it was trained on a small dataset which is not present in glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0623623",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76637e3",
   "metadata": {},
   "source": [
    "A skipgram is where we use target word to predict context word. The will be 1 to 1 mapping for words with target words repeating multiple time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3d52b",
   "metadata": {},
   "source": [
    "### Data creation for Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476a69c",
   "metadata": {},
   "source": [
    "We will reuse the datastructure we built for CBOW and reformat it for skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a99ef919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video',\n",
       " 'large',\n",
       " 'number',\n",
       " 'view',\n",
       " 'large',\n",
       " 'number',\n",
       " 'million',\n",
       " 'people',\n",
       " 'earth']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38e239a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_corpus(train_corpus):\n",
    "    for i in train_corpus:\n",
    "        target_list = []\n",
    "        context_list = []\n",
    "        for idx,j in enumerate(i):\n",
    "            if len(i) > 3:\n",
    "                #print(j)\n",
    "                if idx == 0 :\n",
    "                    #print(000)\n",
    "                    context_list.append(i[idx + 1])\n",
    "                    context_list.append(i[idx + 2])\n",
    "                    target_list += [j]*2\n",
    "                elif idx == 1 :\n",
    "                    #print(111)\n",
    "                    context_list.append(i[idx - 1])\n",
    "                    context_list.append(i[idx + 1])\n",
    "                    context_list.append(i[idx + 2])\n",
    "                    target_list += [j]*3\n",
    "                elif idx == (len(i) - 1):\n",
    "                    #print(222)\n",
    "                    context_list.append(i[idx - 1])\n",
    "                    context_list.append(i[idx - 2])\n",
    "                    target_list += [j]*2\n",
    "                elif idx == (len(i) - 2):\n",
    "                    #print(222)\n",
    "                    context_list.append(i[idx - 1])\n",
    "                    context_list.append(i[idx - 2])\n",
    "                    context_list.append(i[idx + 1])\n",
    "                    target_list += [j]*3\n",
    "                elif (idx >= 2) and (idx < (len(i) - 2)):\n",
    "                    #print(333)\n",
    "                    context_list.append(i[idx - 2])\n",
    "                    context_list.append(i[idx - 1])\n",
    "                    context_list.append(i[idx + 1])\n",
    "                    context_list.append(i[idx + 2])\n",
    "\n",
    "                    target_list += [j] * 4\n",
    "                else:\n",
    "                    #print(444)\n",
    "                    context_list.append(i[idx - 1])\n",
    "                    context_list.append(i[idx + 1])\n",
    "                    target_list += [j] * 2\n",
    "            else:\n",
    "                pass\n",
    "    return(target_list,context_list)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c25ab5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_skip, context_skip = create_skipgram_corpus(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c68c1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_num(list_wrds,word_dict):\n",
    "    skip_num = []\n",
    "    for i in list_wrds:\n",
    "        if i in  word_dict:\n",
    "            skip_num.append(word_dict[i])\n",
    "        else:\n",
    "            skip_num.append(1)\n",
    "    return(skip_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a174d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_skip_num = convert_list_to_num(target_skip,filtered_all_words)\n",
    "context_skip_num = convert_list_to_num(context_skip,filtered_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e8e1dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15, 1, 1, 15, 4, 1, 5, 4, 1, 5, 3, 1, 1, 3, 5, 1, 5, 5, 274, 5, 3, 274, 4]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_skip_num[1:25]\n",
    "context_skip_num[1:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4de099",
   "metadata": {},
   "source": [
    "<b> Let us create a dictionary for a target and label matching vector </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3204caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensive_dict = {}\n",
    "for i,j in zip(target_skip_num,context_skip_num):\n",
    "    if i not in comprehensive_dict:\n",
    "        comprehensive_dict[i] = [j]\n",
    "    else:\n",
    "        comprehensive_dict[i].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "153272a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 -> <class 'int'>\n",
      "1 -> <class 'int'>\n",
      "1 -> <class 'int'>\n",
      "5 -> <class 'int'>\n",
      "274 -> <class 'int'>\n",
      "24 -> <class 'int'>\n",
      "24 -> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for element in comprehensive_dict[4]:\n",
    "    print(f'{element} -> {type(element)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4e845",
   "metadata": {},
   "source": [
    "### Creating a pytorch training dataset for Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "713fea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_skip_tensor = torch.tensor(target_skip_num, dtype = torch.long)\n",
    "context_skip_tensor = torch.tensor(context_skip_num, dtype = torch.long)\n",
    "\n",
    "dataset_train_skip = TensorDataset(target_skip_tensor, context_skip_tensor)\n",
    "dataloader_train_skip = DataLoader(dataset_train_skip, batch_size=10, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d450c",
   "metadata": {},
   "source": [
    "### Build a Skipgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "72df5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, center):\n",
    "        embed = self.embedding(center)\n",
    "        out = self.output(embed)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "90177752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 46.7078\n",
      "Epoch 2, Loss: 37.9226\n",
      "Epoch 3, Loss: 30.0930\n",
      "Epoch 4, Loss: 22.8505\n",
      "Epoch 5, Loss: 17.7048\n",
      "Epoch 6, Loss: 14.8496\n",
      "Epoch 7, Loss: 13.4327\n",
      "Epoch 8, Loss: 12.7436\n",
      "Epoch 9, Loss: 12.3577\n",
      "Epoch 10, Loss: 12.1121\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(filtered_all_words) + 1\n",
    "embed_dim = 50\n",
    "\n",
    "model_skip = SkipGramModel(vocab_size, embed_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_skip.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for center_batch, context_batch in dataloader_train_skip:\n",
    "        output = model_skip(center_batch)  \n",
    "        \n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        loss = criterion(output, context_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d5382588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram prediction accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for center, context in dataloader_train_skip:\n",
    "        #print(center)\n",
    "        output = model_skip(center)  \n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        for inx,targ in enumerate(center):\n",
    "            val = int(int(preds[inx]) in comprehensive_dict[int(targ)])\n",
    "            correct += val\n",
    "            #print(\"prediction\",preds[inx])\n",
    "            #print(\"actual\",comprehensive_dict[int(targ)])\n",
    "        #correct += (preds == context).sum().item()\n",
    "        total += context.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Skip-gram prediction accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e1e02f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(torch.tensor([15, 15,  4,  4,  4,  1,  1,  1,  1,  1])):\n",
    "    print(float(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786c8ad",
   "metadata": {},
   "source": [
    "# Get word embeddings for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e1842cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train_vector_skip = []\n",
    "for i in padded_train_corpus_encoded:\n",
    "    sent = []\n",
    "    for j in i:\n",
    "        #print(j)\n",
    "        if j == 0:\n",
    "            emb_word = torch.tensor(np.zeros(50))\n",
    "        else:\n",
    "            emb_word = (model_skip.embedding(torch.tensor([j-1])).squeeze(0).tolist())\n",
    "        sent.append(emb_word)\n",
    "    embed_train_vector_skip.append(sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e6276dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_test_vector_skip = []\n",
    "for i in padded_test_corpus_encoded:\n",
    "    sent = []\n",
    "    for j in i:\n",
    "        #print(j)\n",
    "        if j == 0:\n",
    "            emb_word = torch.tensor(np.zeros(50))\n",
    "        else:\n",
    "            emb_word = (model_skip.embedding(torch.tensor([j-1])).squeeze(0).tolist())\n",
    "        sent.append(emb_word)\n",
    "    embed_test_vector_skip.append(sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5d1a9847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1564, 12, 50])\n",
      "torch.Size([1564, 50, 12])\n",
      "torch.Size([392, 12, 50])\n",
      "torch.Size([392, 50, 12])\n"
     ]
    }
   ],
   "source": [
    "X_tensor_train_skip = torch.tensor(embed_train_vector_skip, dtype=torch.float32)    \n",
    "print(X_tensor_train_skip.shape)\n",
    "\n",
    "#The expected sequence of nn.Conv1d is (batch_size, channels, sequence_length)\n",
    "# hence we change the shape (1564, 15, 50) to (1564, 50, 15)\n",
    "X_tensor_train_skip = X_tensor_train_skip.permute(0, 2, 1)\n",
    "print(X_tensor_train_skip.shape)\n",
    "\n",
    "y_tensor_train = torch.tensor(np.array(y_train), dtype=torch.long)\n",
    "\n",
    "dataset_train_skip_final = TensorDataset(X_tensor_train_skip, y_tensor_train)\n",
    "dataloader_train_skip_final = DataLoader(dataset_train_skip_final, batch_size=10, shuffle=False,drop_last=False)\n",
    "\n",
    "\n",
    "X_tensor_test_skip = torch.tensor(embed_test_vector_skip, dtype=torch.float32)    \n",
    "print(X_tensor_test_skip.shape)\n",
    "\n",
    "#The expected sequence of nn.Conv1d is (batch_size, channels, sequence_length)\n",
    "# hence we change the shape (1564, 15, 50) to (1564, 50, 15)\n",
    "X_tensor_test_skip = X_tensor_test_skip.permute(0, 2, 1)\n",
    "print(X_tensor_test_skip.shape)\n",
    "\n",
    "y_tensor_test = torch.tensor(np.array(y_test), dtype=torch.long)\n",
    "\n",
    "dataset_test_skip_final = TensorDataset(X_tensor_test_skip, y_tensor_test)\n",
    "dataloader_test_skip_final = DataLoader(dataset_test_skip_final, batch_size=10, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "092e936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17676285383807624\n",
      "0.17378935084980765\n",
      "0.20718121243889925\n",
      "0.17680333602200649\n",
      "0.17288797714148357\n",
      "0.16919684694830778\n",
      "0.17604487640842512\n",
      "0.17746795666445592\n",
      "0.17357014280975244\n",
      "0.1728519189889264\n",
      "0.17319917982550942\n",
      "0.17321116567417316\n",
      "0.16529188471235287\n",
      "0.17784722329704625\n",
      "0.16098718468550663\n",
      "0.15442916826837383\n",
      "0.15604617944948232\n",
      "0.16263728422723758\n",
      "0.16356975884194586\n",
      "0.1582968214135261\n",
      "0.15384659417875254\n",
      "0.1447944176045193\n",
      "0.13393385490034795\n",
      "0.12408586634192498\n",
      "0.10846863478232341\n",
      "0.10480645023713446\n",
      "0.09372278442902929\n",
      "0.08625432363693501\n",
      "0.06537670474855384\n",
      "0.05481062842248257\n",
      "0.054647522492061375\n",
      "0.05549473755334498\n",
      "0.05533438702084267\n",
      "0.036883278067704216\n",
      "0.029119872382047118\n",
      "0.026307348726216442\n",
      "0.024907106658959892\n",
      "0.0357026521192128\n",
      "0.015942147796510892\n",
      "0.011890258048890918\n"
     ]
    }
   ],
   "source": [
    "model3 = LSTM_spam_classify()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=0.001)\n",
    "train_losses = []\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    model3.train()\n",
    "    for inputs, labels in dataloader_test_skip_final:\n",
    "        #inputs = inputs.float()\n",
    "        #labels = labels.float()\n",
    "        #inputs = inputs.permute(0,2,1)\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        outputs = model3(inputs)\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(outputs, labels.squeeze(1))\n",
    "        #print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #get_norm()\n",
    "        torch.nn.utils.clip_grad_norm_(model3.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(dataloader_train)\n",
    "    print(avg_train_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    #print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7de4ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1564\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "# Predictions on training data\n",
    "\n",
    "model3.eval() \n",
    "\n",
    "train_preds = []\n",
    "actual = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for inputs, labels in dataloader_train_skip_final:  \n",
    "        actual.append(labels)\n",
    "        outputs_train = model3(inputs) \n",
    "        preds = torch.argmax(outputs_train, dim=1) \n",
    "        train_preds.extend(preds.numpy())\n",
    "print(len(train_preds))\n",
    "print(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "182d2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.eval() \n",
    "\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for inputs, labels in dataloader_test_skip_final:  \n",
    "        outputs_test = model3(inputs) \n",
    "        preds = torch.argmax(outputs_test, dim=1) \n",
    "        test_preds.extend(preds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f8ba628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy is 0.754\n",
      "test accuracy is 0.99\n",
      "training precision is 0.772\n",
      "test precision is 0.985\n",
      "training recall is 0.741\n",
      "test recall is 0.995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75       760\n",
      "           1       0.77      0.74      0.76       804\n",
      "\n",
      "    accuracy                           0.75      1564\n",
      "   macro avg       0.75      0.75      0.75      1564\n",
      "weighted avg       0.76      0.75      0.75      1564\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       191\n",
      "           1       0.99      1.00      0.99       201\n",
      "\n",
      "    accuracy                           0.99       392\n",
      "   macro avg       0.99      0.99      0.99       392\n",
      "weighted avg       0.99      0.99      0.99       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "cnn_train_accuracy = accuracy_score(y_train, train_preds)\n",
    "# Testing accuracy\n",
    "cnn_test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "cnn_train_precision = precision_score(y_train, train_preds)\n",
    "cnn_test_precision = precision_score(y_test, test_preds)\n",
    "\n",
    "cnn_train_recall = recall_score(y_train, train_preds)\n",
    "cnn_test_recall = recall_score(y_test, test_preds)\n",
    "\n",
    "print(\"training accuracy is {0}\".format(round(cnn_train_accuracy,3)))\n",
    "print(\"test accuracy is {0}\".format(round(cnn_test_accuracy,3)))\n",
    "\n",
    "print(\"training precision is {0}\".format(round(cnn_train_precision,3)))\n",
    "print(\"test precision is {0}\".format(round(cnn_test_precision,3)))\n",
    "\n",
    "print(\"training recall is {0}\".format(round(cnn_train_recall,3)))\n",
    "print(\"test recall is {0}\".format(round(cnn_test_recall,3)))\n",
    "\n",
    "print(classification_report(y_train, train_preds))\n",
    "print(classification_report(y_test, test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
